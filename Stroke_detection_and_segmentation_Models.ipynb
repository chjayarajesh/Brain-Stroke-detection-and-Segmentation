{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "authorship_tag": "ABX9TyO7/yGz63mYLnsu+SSRyDbI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chjayarajesh/Brain-Stroke-detection-and-Segmentation/blob/main/Stroke_detection_and_segmentation_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Brain Stroke classification and segmentation**"
      ],
      "metadata": {
        "id": "Y4cQIIfNkHGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using Kaggle API to import dataset**"
      ],
      "metadata": {
        "id": "RhDyvyFDi7K5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxmYnr38hDt0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "68494711-42b2-49bf-98de-1e61ed250c00"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-63fdafd1-b5a7-447b-828d-b181cffa936d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-63fdafd1-b5a7-447b-828d-b181cffa936d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!pip install -q kaggle timm\n",
        "\n",
        "# Upload kaggle.json from your Kaggle account (Account → API → Create New Token)\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "# Move kaggle.json to correct path\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading Dataset"
      ],
      "metadata": {
        "id": "RGINQi4DjBp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Brain Stroke CT Dataset\n",
        "!kaggle datasets download -d ozguraslank/brain-stroke-ct-dataset -p /content/dataset\n",
        "\n",
        "# Unzip the dataset\n",
        "!unzip -q /content/dataset/brain-stroke-ct-dataset.zip -d /content/dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaLietIahcHB",
        "outputId": "3e133853-3598-4027-918a-d88d8b64b5f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/ozguraslank/brain-stroke-ct-dataset\n",
            "License(s): other\n",
            "Downloading brain-stroke-ct-dataset.zip to /content/dataset\n",
            "100% 1.41G/1.41G [00:16<00:00, 165MB/s]\n",
            "100% 1.41G/1.41G [00:16<00:00, 94.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Segregation and splitting into (Train, validate and test)**"
      ],
      "metadata": {
        "id": "CIKaKKGv_FKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2  # Added import for OpenCV\n",
        "\n",
        "# Step 1: Segregate images and overlays into train, val, test\n",
        "def segregate_dataset(base_dir, classes, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
        "    for cls in classes:\n",
        "        png_dir = os.path.join(base_dir, cls, 'PNG')\n",
        "        overlay_dir = os.path.join(base_dir, cls, 'OVERLAY')\n",
        "        if not os.path.exists(png_dir):\n",
        "            print(f\"PNG folder not found for {cls}. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        # Get list of PNG images\n",
        "        images = [os.path.join(png_dir, f) for f in os.listdir(png_dir) if f.lower().endswith('.png')]\n",
        "        if len(images) == 0:\n",
        "            print(f\"No PNG images found for {cls}. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        # Split into train/val/test\n",
        "        train_images, test_images = train_test_split(images, test_size=val_ratio + test_ratio, random_state=42)\n",
        "        val_images, test_images = train_test_split(test_images, test_size=test_ratio / (val_ratio + test_ratio), random_state=42)\n",
        "\n",
        "        # Create directories for PNG and OVERLAY\n",
        "        for split, split_images in [('train', train_images), ('val', val_images), ('test', test_images)]:\n",
        "            split_png_dir = os.path.join(base_dir, split, 'PNG', cls)\n",
        "            split_overlay_dir = os.path.join(base_dir, split, 'OVERLAY', cls)\n",
        "            os.makedirs(split_png_dir, exist_ok=True)\n",
        "            os.makedirs(split_overlay_dir, exist_ok=True)\n",
        "\n",
        "            # Copy PNG images\n",
        "            for img in split_images:\n",
        "                filename = os.path.basename(img)\n",
        "                shutil.copy(img, os.path.join(split_png_dir, filename))\n",
        "\n",
        "            # Copy corresponding OVERLAY images or create zero mask for Normal\n",
        "            for img in split_images:\n",
        "                filename = os.path.basename(img)\n",
        "                overlay_path = os.path.join(overlay_dir, filename)\n",
        "                if os.path.exists(overlay_path):\n",
        "                    shutil.copy(overlay_path, os.path.join(split_overlay_dir, filename))\n",
        "                elif cls == 'Normal' and not os.path.exists(overlay_dir):\n",
        "                    # Create a zero mask for Normal if no overlay exists\n",
        "                    zero_mask = np.zeros((256, 256), dtype=np.uint8)  # Adjust size if needed\n",
        "                    cv2.imwrite(os.path.join(split_overlay_dir, filename), zero_mask)\n",
        "                    print(f\"Created zero mask for {filename} in {cls}\")\n",
        "                else:\n",
        "                    print(f\"Overlay not found for {filename}, skipping copy.\")\n",
        "\n",
        "        print(f\"{cls}: Train {len(train_images)}, Val {len(val_images)}, Test {len(test_images)}\")\n",
        "\n",
        "# Base directory (adjust to your Colab path)\n",
        "base_dir = '/content/dataset/Brain_Stroke_CT_Dataset'  # Update to your actual path\n",
        "classes = ['Bleeding', 'Ischemia', 'Normal']  # Adjust based on your dataset\n",
        "\n",
        "# Run segregation\n",
        "segregate_dataset(base_dir, classes)"
      ],
      "metadata": {
        "id": "oW498DegstQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Classification Model training**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Preproccesing and augmentation of dataset\n",
        "\n",
        "---\n",
        "\n",
        "ConvNext model -(Convolutional Next)\n"
      ],
      "metadata": {
        "id": "A_Nm5MdejFoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import timm\n",
        "from tqdm import tqdm  # For progress bar\n",
        "\n",
        "base_dir = '/content/dataset/Brain_Stroke_CT_Dataset'  # Update to your actual path in Colab\n",
        "\n",
        "# Step 2: Preprocessing and Training\n",
        "# Custom transform to replicate grayscale to 3 channels\n",
        "class GrayscaleToRGB:\n",
        "    def __call__(self, image):\n",
        "        if image.shape[0] == 1:\n",
        "            return torch.cat([image] * 3, dim=0)\n",
        "        return image\n",
        "\n",
        "# Custom Dataset with 3-channel replication\n",
        "class CTDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        self.class_names = sorted(os.listdir(data_dir))\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.class_names)}\n",
        "\n",
        "        for cls_name in self.class_names:\n",
        "            cls_dir = os.path.join(data_dir, cls_name)\n",
        "            if os.path.isdir(cls_dir):\n",
        "                for img_name in os.listdir(cls_dir):\n",
        "                    img_path = os.path.join(cls_dir, img_name)\n",
        "                    if os.path.isfile(img_path):\n",
        "                        self.images.append(img_path)\n",
        "                        self.labels.append(self.class_to_idx[cls_name])\n",
        "        if not self.images:\n",
        "            raise ValueError(f\"No images found in {data_dir}. Check your dataset path and ensure it contains image subfolders.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('L')  # Load as grayscale\n",
        "            if self.transform:\n",
        "                image = self.transform(image)  # Apply full transform pipeline\n",
        "            return image, label\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            return torch.zeros(3, 224, 224), label  # Return 3-channel placeholder\n",
        "\n",
        "# Load datasets (after segregation, use the train/val/test directories)\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir = os.path.join(base_dir, 'val')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    GrayscaleToRGB(),  # Replicate to 3 channels after ToTensor\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # 3-channel normalization\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    GrayscaleToRGB(),  # Replicate to 3 channels after ToTensor\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "train_dataset = CTDataset(train_dir, transform=train_transform)\n",
        "val_dataset = CTDataset(val_dir, transform=val_test_transform)\n",
        "test_dataset = CTDataset(test_dir, transform=val_test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
        "print(f\"Number of test samples: {len(test_dataset)}\")\n",
        "print(f\"Classes: {train_dataset.class_names}\")\n",
        "\n",
        "# Set device\n",
        "torch.cuda.empty_cache()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Load pre-trained ConvNeXt-Base model\n",
        "model = timm.create_model('convnext_base', pretrained=True)\n",
        "model.reset_classifier(num_classes=3)  # 3 classes: hemorrhagic, ischemic, normal\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
        "    best_acc = 0.0\n",
        "    val_accuracies = []\n",
        "    val_precisions = []\n",
        "    val_recalls = []\n",
        "    val_f1_scores = []\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        print(f\"Starting epoch {epoch + 1}/{num_epochs}\")\n",
        "        with tqdm(total=len(train_loader), desc=f\"Epoch {epoch + 1}/{num_epochs}\") as pbar:\n",
        "            for i, (inputs, labels) in enumerate(train_loader):\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "                pbar.update(1)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        epoch_val_loss = val_loss / len(val_loader)\n",
        "        epoch_acc = accuracy_score(all_labels, all_preds)\n",
        "        epoch_precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "        epoch_recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "        epoch_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "        val_accuracies.append(epoch_acc)\n",
        "        val_precisions.append(epoch_precision)\n",
        "        val_recalls.append(epoch_recall)\n",
        "        val_f1_scores.append(epoch_f1)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_acc:.4f}, Precision: {epoch_precision:.4f}, Recall: {epoch_recall:.4f}, F1: {epoch_f1:.4f}')\n",
        "\n",
        "        if epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            torch.save(model.state_dict(), 'best_model_3class.pth')\n",
        "\n",
        "    # Plot graphs for 4 parameters\n",
        "    epochs = range(1, num_epochs + 1)\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
        "    fig.suptitle('Validation Metrics Over Epochs')\n",
        "\n",
        "    # Accuracy graph\n",
        "    axs[0, 0].plot(epochs, val_accuracies, label='Accuracy', color='b')\n",
        "    axs[0, 0].set_title('Accuracy')\n",
        "    axs[0, 0].set_xlabel('Epoch')\n",
        "    axs[0, 0].set_ylabel('Score')\n",
        "    axs[0, 0].legend()\n",
        "\n",
        "    # Precision graph\n",
        "    axs[0, 1].plot(epochs, val_precisions, label='Precision', color='r')\n",
        "    axs[0, 1].set_title('Precision')\n",
        "    axs[0, 1].set_xlabel('Epoch')\n",
        "    axs[0, 1].set_ylabel('Score')\n",
        "    axs[0, 1].legend()\n",
        "\n",
        "    # Recall graph\n",
        "    axs[1, 0].plot(epochs, val_recalls, label='Recall', color='g')\n",
        "    axs[1, 0].set_title('Recall')\n",
        "    axs[1, 0].set_xlabel('Epoch')\n",
        "    axs[1, 0].set_ylabel('Score')\n",
        "    axs[1, 0].legend()\n",
        "\n",
        "    # F1 Score graph\n",
        "    axs[1, 1].plot(epochs, val_f1_scores, label='F1 Score', color='m')\n",
        "    axs[1, 1].set_title('F1 Score')\n",
        "    axs[1, 1].set_xlabel('Epoch')\n",
        "    axs[1, 1].set_ylabel('Score')\n",
        "    axs[1, 1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('validation_metrics_graphs.png')  # Save the figure\n",
        "    plt.show()\n",
        "    return val_accuracies, val_precisions, val_recalls, val_f1_scores\n",
        "\n",
        "# Evaluate function\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n",
        "    print(f'Precision: {precision:.4f}')\n",
        "    print(f'Recall: {recall:.4f}')\n",
        "    print(f'F1 Score: {f1:.4f}')\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Hemorrhagic', 'Ischemic', 'Normal'], yticklabels=['Hemorrhagic', 'Ischemic', 'Normal'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.savefig('confusion_matrix.png')  # Save the confusion matrix\n",
        "    plt.show()\n",
        "\n",
        "# Train the model and get metrics\n",
        "val_accuracies, val_precisions, val_recalls, val_f1_scores = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)\n",
        "\n",
        "# Load best model and evaluate\n",
        "model.load_state_dict(torch.load('best_model_3class.pth'))\n",
        "evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "id": "_BFU8C-HerQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Segmentation Model training**\n",
        "---\n",
        "\n",
        "\n",
        "Preproccesing and augmentation of dataset\n",
        "\n",
        "---\n",
        "\n",
        "U-Net with efficientnet-b4 encoder"
      ],
      "metadata": {
        "id": "yTQb7Szvfzcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install required packages\n",
        "!pip install segmentation-models-pytorch -q\n",
        "\n",
        "# Step 3: Preprocessing and Training\n",
        "import os\n",
        "import cv2\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import torch\n",
        "import torch.optim as optim  # Added import\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import segmentation_models_pytorch as smp\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Configuration\n",
        "class Config:\n",
        "    BATCH_SIZE = 8\n",
        "    IMG_HEIGHT = 256\n",
        "    IMG_WIDTH = 256\n",
        "    EPOCHS = 50\n",
        "    LEARNING_RATE = 1e-4\n",
        "    DATASET_PATH = '/content/brain-stroke-images'  # Root path\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    NUM_WORKERS = 2\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# Preprocessing: Generate pseudo-masks for stroke images\n",
        "def generate_pseudo_mask(image, threshold=0.1):  # Simple thresholding\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "    _, mask = cv2.threshold(gray, threshold * 255, 255, cv2.THRESH_BINARY)\n",
        "    mask = mask / 255.0\n",
        "    return mask.astype(np.float32)\n",
        "\n",
        "# Load data from cropped folders\n",
        "def load_segmentation_data(base_dir):\n",
        "    image_paths = []\n",
        "    mask_paths = []  # Pseudo-masks generated on-the-fly\n",
        "    labels = []  # 1 for STROKE, 0 for NORMAL\n",
        "\n",
        "    for split in ['TRAIN_CROP', 'VAL_CROP', 'TEST_CROP']:\n",
        "        for category in ['NORMAL', 'STROKE']:\n",
        "            category_dir = os.path.join(base_dir, 'stroke_cropped/CROPPED', split, category)\n",
        "            if os.path.exists(category_dir):\n",
        "                images = glob(os.path.join(category_dir, '*.jpg'))\n",
        "                for img in images:\n",
        "                    image_paths.append(img)\n",
        "                    mask_paths.append(None)  # Will generate pseudo-mask\n",
        "                    labels.append(1 if category == 'STROKE' else 0)\n",
        "\n",
        "    print(f\"Loaded {len(image_paths)} images: {np.sum(labels)} stroke, {len(labels) - np.sum(labels)} normal.\")\n",
        "    return image_paths, mask_paths, labels\n",
        "\n",
        "image_paths, mask_paths, labels = load_segmentation_data(config.DATASET_PATH)\n",
        "if len(image_paths) == 0:\n",
        "    raise ValueError(\"No images found! Check dataset path.\")\n",
        "\n",
        "# Split into train/val/test\n",
        "train_images, test_images, train_labels, test_labels = train_test_split(image_paths, labels, test_size=0.2, random_state=42, stratify=labels)\n",
        "val_images, test_images, val_labels, test_labels = train_test_split(test_images, test_labels, test_size=0.5, random_state=42, stratify=test_labels)\n",
        "\n",
        "print(f\"Training samples: {len(train_images)}\")\n",
        "print(f\"Validation samples: {len(val_images)}\")\n",
        "print(f\"Test samples: {len(test_images)}\")\n",
        "\n",
        "# Dataset class\n",
        "class StrokeDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transforms=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = cv2.imread(self.image_paths[idx])\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, (config.IMG_WIDTH, config.IMG_HEIGHT))\n",
        "\n",
        "        label = self.labels[idx]\n",
        "        if label == 1:  # Stroke: generate pseudo-mask\n",
        "            msk = generate_pseudo_mask(img, threshold=0.1)\n",
        "        else:  # Normal: zero mask\n",
        "            msk = np.zeros((config.IMG_HEIGHT, config.IMG_WIDTH), dtype=np.float32)\n",
        "\n",
        "        if self.transforms:\n",
        "            transformed = self.transforms(image=img, mask=msk)\n",
        "            img, msk = transformed['image'], transformed['mask']\n",
        "\n",
        "        return img, msk\n",
        "\n",
        "# Transforms\n",
        "def get_transforms(is_training=True):\n",
        "    if is_training:\n",
        "        return A.Compose([\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.RandomBrightnessContrast(brightness_limit=0.05, contrast_limit=0.05, p=0.3),\n",
        "            A.Affine(translate_percent={\"x\": (-0.05, 0.05), \"y\": (-0.05, 0.05)}, scale=(0.95, 1.05), rotate=(-5, 5), p=0.3),\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "    else:\n",
        "        return A.Compose([\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "train_dataset = StrokeDataset(train_images, train_labels, get_transforms(True))\n",
        "val_dataset = StrokeDataset(val_images, val_labels, get_transforms(False))\n",
        "test_dataset = StrokeDataset(test_images, test_labels, get_transforms(False))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=config.NUM_WORKERS, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=config.NUM_WORKERS, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=config.NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "# Segmentation Model\n",
        "model = smp.Unet(\n",
        "    encoder_name='efficientnet-b4',\n",
        "    encoder_weights='imagenet',\n",
        "    in_channels=3,\n",
        "    classes=1,\n",
        "    activation=None,\n",
        ").to(config.DEVICE)\n",
        "\n",
        "# Loss and Optimizer\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, smooth=1e-6):\n",
        "        super().__init__()\n",
        "        self.smooth = smooth\n",
        "    def forward(self, y_pred_prob, y_true):\n",
        "        y_pred = y_pred_prob.view(-1)\n",
        "        y_true = y_true.view(-1)\n",
        "        intersection = (y_pred * y_true).sum()\n",
        "        dice = (2. * intersection + self.smooth) / (y_pred.sum() + y_true.sum() + self.smooth)\n",
        "        return 1 - dice\n",
        "\n",
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.bce = nn.BCEWithLogitsLoss()\n",
        "        self.dice = DiceLoss()\n",
        "    def forward(self, y_pred_logits, y_true):\n",
        "        prob = torch.sigmoid(y_pred_logits)\n",
        "        return 0.5 * self.bce(y_pred_logits, y_true) + 0.5 * self.dice(prob, y_true)\n",
        "\n",
        "criterion = CombinedLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)  # Now should work\n",
        "\n",
        "# Training\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = running_dice = running_iou = 0.0\n",
        "    progress_bar = tqdm(dataloader, desc='Training')\n",
        "    for batch_idx, (images, masks) in enumerate(progress_bar):\n",
        "        images = images.to(device)\n",
        "        masks = masks.unsqueeze(1).to(device)\n",
        "\n",
        "        logits = model(images)\n",
        "        loss = criterion(logits, masks)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        probs = torch.sigmoid(logits)\n",
        "        dice_score = calculate_dice_score(probs, masks)\n",
        "        iou_score = calculate_iou_score(probs, masks)\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        running_dice += dice_score\n",
        "        running_iou += iou_score\n",
        "\n",
        "        progress_bar.set_postfix({'Loss': f'{loss.item():.4f}', 'Dice': f'{dice_score:.4f}', 'IoU': f'{iou_score:.4f}'})\n",
        "\n",
        "    N = len(dataloader)\n",
        "    return running_loss / N, running_dice / N, running_iou / N\n",
        "\n",
        "def validate_epoch(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = running_dice = running_iou = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in dataloader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.unsqueeze(1).to(device)\n",
        "\n",
        "            logits = model(images)\n",
        "            loss = criterion(logits, masks)\n",
        "\n",
        "            probs = torch.sigmoid(logits)\n",
        "            dice_score = calculate_dice_score(probs, masks)\n",
        "            iou_score = calculate_iou_score(probs, masks)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            running_dice += dice_score\n",
        "            running_iou += iou_score\n",
        "\n",
        "    N = len(dataloader)\n",
        "    return running_loss / N, running_dice / N, running_iou / N\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=50):\n",
        "    best_dice = 0.0\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "        train_loss, train_dice, train_iou = train_epoch(model, train_loader, criterion, optimizer, config.DEVICE)\n",
        "        val_loss, val_dice, val_iou = validate_epoch(model, val_loader, criterion, config.DEVICE)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Dice: {train_dice:.4f}, Train IoU: {train_iou:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val Dice: {val_dice:.4f}, Val IoU: {val_iou:.4f}\")\n",
        "\n",
        "        if val_dice > best_dice:\n",
        "            best_dice = val_dice\n",
        "            torch.save(model.state_dict(), 'best_stroke_segmentation_model.pth')\n",
        "            print(f\"New best model saved! Dice: {best_dice:.4f}\")\n",
        "\n",
        "    model.load_state_dict(torch.load('best_stroke_segmentation_model.pth'))\n",
        "    return model\n",
        "\n",
        "# Metrics\n",
        "def calculate_dice_score(y_pred_prob, y_true, smooth=1e-6):\n",
        "    y_pred = (y_pred_prob > 0.5).float()\n",
        "    intersection = (y_pred * y_true).sum()\n",
        "    dice = (2. * intersection + smooth) / (y_pred.sum() + y_true.sum() + smooth)\n",
        "    return dice.item()\n",
        "\n",
        "def calculate_iou_score(y_pred_prob, y_true, smooth=1e-6):\n",
        "    y_pred = (y_pred_prob > 0.5).float()\n",
        "    intersection = (y_pred * y_true).sum()\n",
        "    union = y_pred.sum() + y_true.sum() - intersection\n",
        "    iou = (intersection + smooth) / (union + smooth)\n",
        "    return iou.item()\n",
        "\n",
        "# Train the model\n",
        "model = train_model(model, train_loader, val_loader, criterion, optimizer)\n",
        "\n",
        "# Step 3: Show 2 Images After Segmentation\n",
        "def predict_and_show(model, image_paths, num_samples=2):\n",
        "    model.eval()\n",
        "    transforms = get_transforms(False)\n",
        "    for i in range(num_samples):\n",
        "        image = cv2.imread(image_paths[i])\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image_resized = cv2.resize(image_rgb, (config.IMG_WIDTH, config.IMG_HEIGHT))\n",
        "\n",
        "        transformed = transforms(image=image_resized, mask=np.zeros((config.IMG_HEIGHT, config.IMG_WIDTH), np.float32))\n",
        "        image_tensor = transformed['image'].unsqueeze(0).to(config.DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred_logits = model(image_tensor)[0, 0].cpu().numpy()\n",
        "            pred_prob = 1 / (1 + np.exp(-pred_logits))\n",
        "            pred_mask_binary = (pred_prob > 0.5).astype(np.uint8) * 255\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.imshow(pred_mask_binary, cmap='gray')\n",
        "        plt.title(f\"Segmented Image {i+1}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "# Use 2 images from test set\n",
        "predict_and_show(model, test_images, num_samples=2)"
      ],
      "metadata": {
        "id": "tY5Ht8CGf6SY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}